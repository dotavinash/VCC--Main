{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dotavinash/VCC--Main/blob/main/Assignment2_G24AI2025_AvinashKumar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install mrjob\n",
        "!pip install mrjob\n",
        "\n",
        "# Install Java (required for Hadoop)\n",
        "!apt-get install openjdk-8-jdk -qq > /dev/null\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "\n",
        "# Download and setup Hadoop (using a recent stable version, e.g., 3.3.6)\n",
        "HADOOP_VERSION = \"3.3.6\"\n",
        "!wget -q https://archive.apache.org/dist/hadoop/common/hadoop-{HADOOP_VERSION}/hadoop-{HADOOP_VERSION}.tar.gz\n",
        "!tar -xzf hadoop-{HADOOP_VERSION}.tar.gz\n",
        "!mv hadoop-{HADOOP_VERSION} /usr/local/hadoop\n",
        "\n",
        "# Set Hadoop environment variables\n",
        "os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop\"\n",
        "os.environ[\"PATH\"] = os.environ[\"PATH\"] + \":\" + os.environ[\"HADOOP_HOME\"] + \"/bin\"\n",
        "\n",
        "# Handle CLASSPATH: Check if it exists before appending, otherwise initialize it.\n",
        "if \"CLASSPATH\" in os.environ:\n",
        "    os.environ[\"CLASSPATH\"] = os.environ[\"CLASSPATH\"] + \":\" + os.environ[\"HADOOP_HOME\"] + \"/lib/*\"\n",
        "else:\n",
        "    os.environ[\"CLASSPATH\"] = os.environ[\"HADOOP_HOME\"] + \"/lib/*\"\n",
        "\n",
        "print(\"Added dependencies !\")"
      ],
      "metadata": {
        "id": "fhfhfyY84l5E",
        "outputId": "22347a71-032b-431f-b08e-9862cb4cbf45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mrjob\n",
            "  Downloading mrjob-0.7.4-py2.py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.11/dist-packages (from mrjob) (6.0.2)\n",
            "Downloading mrjob-0.7.4-py2.py3-none-any.whl (439 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/439.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.1/439.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.6/439.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mrjob\n",
            "Successfully installed mrjob-0.7.4\n",
            "Added dependencies !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Source URLs for datasets ---\n",
        "URL_CRUISE_DATA = \"https://raw.githubusercontent.com/TakMashhido/PGD-BigData-Tutorial/refs/heads/main/Dataset/cruise.csv\"\n",
        "URL_CHURN_DATA = \"https://raw.githubusercontent.com/TakMashhido/PGD-BigData-Tutorial/refs/heads/main/Dataset/customer_churn.csv\"\n",
        "URL_ECOM_DATA = \"https://raw.githubusercontent.com/TakMashhido/PGD-BigData-Tutorial/refs/heads/main/Dataset/e-com_customer.csv\"\n",
        "\n",
        "# --- Filenames for saving the datasets locally ---\n",
        "FILENAME_CRUISE = \"cruise_data.csv\"\n",
        "FILENAME_CHURN = \"churn_data.csv\"\n",
        "FILENAME_ECOM = \"ecommerce_data.csv\"\n",
        "\n",
        "print(\"Initiating download of datasets from GitHub URLs...\")\n",
        "\n",
        "# Fetch cruise dataset\n",
        "!wget -q -O {FILENAME_CRUISE} {URL_CRUISE_DATA}\n",
        "\n",
        "# Fetch churn dataset\n",
        "!wget -q -O {FILENAME_CHURN} {URL_CHURN_DATA}\n",
        "\n",
        "# Fetch e-commerce dataset\n",
        "!wget -q -O {FILENAME_ECOM} {URL_ECOM_DATA}\n",
        "\n",
        "print(\"\\nDownload completed. Listing CSV files:\")\n",
        "!ls -lh *.csv\n"
      ],
      "metadata": {
        "id": "r1JgqMha5y5I",
        "outputId": "ab53bd77-6c5d-4cec-b013-35f9394155ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initiating download of datasets from GitHub URLs...\n",
            "\n",
            "Download completed. Listing CSV files:\n",
            "-rw-r--r-- 1 root root 113K Jul 29 15:57 churn_data.csv\n",
            "-rw-r--r-- 1 root root 8.6K Jul 29 15:57 cruise_data.csv\n",
            "-rw-r--r-- 1 root root  85K Jul 29 15:57 ecommerce_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: Cruise Line Aggregations\n",
        "\n",
        "This task involves performing aggregations (e.g., total passengers and tonnage) by cruise line using MapReduce."
      ],
      "metadata": {
        "id": "Ph89UUFx59zl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile cruise_summary_job.py\n",
        "from mrjob.job import MRJob\n",
        "from mrjob.step import MRStep\n",
        "import csv\n",
        "\n",
        "class CruiseStatsAggregator(MRJob):\n",
        "    \"\"\"\n",
        "    This MRJob calculates:\n",
        "    - Number of ships\n",
        "    - Average tonnage\n",
        "    - Maximum crew size\n",
        "    for each cruise line using MapReduce.\n",
        "    Partial aggregation is handled in the combiner to reduce data shuffle.\n",
        "    \"\"\"\n",
        "\n",
        "    def steps(self):\n",
        "        return [\n",
        "            MRStep(mapper=self.map_cruise_data,\n",
        "                   combiner=self.combine_stats,\n",
        "                   reducer=self.reduce_stats)\n",
        "        ]\n",
        "\n",
        "    def map_cruise_data(self, _, line):\n",
        "        \"\"\"\n",
        "        Mapper function:\n",
        "        Processes CSV input lines and emits:\n",
        "        (CruiseLine, (1, Tonnage, CrewSize)).\n",
        "        \"\"\"\n",
        "        if line.startswith(\"Cruise_line\"):\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            row = next(csv.reader([line]))\n",
        "            cruise_line = row[0]\n",
        "            tonnage_val = float(row[2])\n",
        "            crew_val = int(row[4])\n",
        "            yield cruise_line, (1, tonnage_val, crew_val)\n",
        "        except (ValueError, IndexError):\n",
        "            self.increment_counter('CruiseStatsAggregator', 'Invalid Rows', 1)\n",
        "            return\n",
        "\n",
        "    def combine_stats(self, cruise_line, entries):\n",
        "        \"\"\"\n",
        "        Combiner function:\n",
        "        Aggregates intermediate results to reduce network overhead.\n",
        "        Adds ship counts and tonnage; tracks max crew size.\n",
        "        \"\"\"\n",
        "        count = 0\n",
        "        tonnage_total = 0.0\n",
        "        crew_max = 0\n",
        "\n",
        "        for ships, tons, crew in entries:\n",
        "            count += ships\n",
        "            tonnage_total += tons\n",
        "            crew_max = max(crew_max, crew)\n",
        "\n",
        "        yield cruise_line, (count, tonnage_total, crew_max)\n",
        "\n",
        "    def reduce_stats(self, cruise_line, entries):\n",
        "        \"\"\"\n",
        "        Reducer function:\n",
        "        Merges combined results from all mappers.\n",
        "        Computes final ship count, average tonnage, and largest crew size.\n",
        "        \"\"\"\n",
        "        total_ships = 0\n",
        "        total_tons = 0.0\n",
        "        max_crew_seen = 0\n",
        "\n",
        "        for ships, tons, crew in entries:\n",
        "            total_ships += ships\n",
        "            total_tons += tons\n",
        "            max_crew_seen = max(max_crew_seen, crew)\n",
        "\n",
        "        avg_tonnage = total_tons / total_ships if total_ships > 0 else 0.0\n",
        "        yield cruise_line, (total_ships, round(avg_tonnage, 2), max_crew_seen)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    CruiseStatsAggregator.run()\n"
      ],
      "metadata": {
        "id": "O6haOLN25-ln",
        "outputId": "6b1ba94d-5424-45cc-9b9a-97942bb723bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cruise_summary_job.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a mock dataset file for testing the MapReduce job\n",
        "sample_data = \"\"\"Cruise_line,Cruise_ship_name,Tonnage,passengers,crew,built,Inaugural_Date,Years_in_service,Passenger_density,length,cabins\n",
        "AIDA Cruises,AIDAbella,69203,2050,600,2008,2008,12,33.75,252.0,1025\n",
        "AIDA Cruises,AIDAluna,69203,2050,600,2009,2009,11,33.75,252.0,1025\n",
        "Carnival Cruise Line,Carnival Freedom,110000,2974,1150,2007,2007,13,37.00,290.0,1487\n",
        "Carnival Cruise Line,Carnival Horizon,133500,3960,1450,2018,2018,2,33.71,323.0,1980\n",
        "Royal Caribbean,Allure of the Seas,225282,5400,2200,2010,2010,10,41.67,362.0,2700\n",
        "\"\"\"\n",
        "\n",
        "# Save the mock CSV data into a file\n",
        "with open(\"demo_cruise_data.csv\", \"w\") as file:\n",
        "    file.write(sample_data)\n",
        "\n",
        "print(\"Executing cruise summary job on demo_cruise_data.csv:\")\n",
        "\n",
        "# Run the MapReduce script using MRJob\n",
        "!python cruise_summary_job.py demo_cruise_data.csv\n"
      ],
      "metadata": {
        "id": "fee3iMu26N1o",
        "outputId": "e9bc5a3b-3780-4d6d-f943-5686e22a716e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing cruise summary job on demo_cruise_data.csv:\n",
            "No configs found; falling back on auto-configuration\n",
            "No configs specified for inline runner\n",
            "Creating temp directory /tmp/cruise_summary_job.root.20250729.160025.021909\n",
            "Running step 1 of 1...\n",
            "job output is in /tmp/cruise_summary_job.root.20250729.160025.021909/output\n",
            "Streaming final output from /tmp/cruise_summary_job.root.20250729.160025.021909/output...\n",
            "\"Royal Caribbean\"\t[1, 225282.0, 2200]\n",
            "\"AIDA Cruises\"\t[2, 69203.0, 600]\n",
            "\"Carnival Cruise Line\"\t[2, 121750.0, 1450]\n",
            "Removing temp directory /tmp/cruise_summary_job.root.20250729.160025.021909...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile churn_rate_analysis.py\n",
        "from mrjob.job import MRJob\n",
        "from mrjob.step import MRStep\n",
        "import csv\n",
        "\n",
        "class CompanyChurnRateJob(MRJob):\n",
        "    \"\"\"\n",
        "    This job computes the churn percentage for selected companies\n",
        "    by analyzing records in the customer churn dataset.\n",
        "    This job computes the churn percentage for selected companies\n",
        "    by analyzing records in the customer churn dataset.This job computes the churn percentage for selected companies\n",
        "    by analyzing records in the customer churn dataset.This job computes the churn percentage for selected companies\n",
        "    by analyzing records in the customer churn dataset.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def configure_args(self):\n",
        "        super(CompanyChurnRateJob, self).configure_args()\n",
        "        self.add_file_arg('--vip-companies', help='File containing names of VIP companies')\n",
        "\n",
        "    def load_target_companies(self):\n",
        "        \"\"\"\n",
        "        This runs once per mapper instance and reads the list of target companies\n",
        "        from a file passed via --vip-companies argument.\n",
        "\n",
        "        This runs once per mapper instance and reads the list of target companies\n",
        "        from a file passed via --vip-companies argument.\n",
        "\n",
        "        This runs once per mapper instance and reads the list of target companies\n",
        "        from a file passed via --vip-companies argument.\n",
        "        \"\"\"\n",
        "        self.target_companies = set()\n",
        "        if self.options.vip_companies:\n",
        "            with open(self.options.vip_companies, 'r') as file:\n",
        "                for line in file:\n",
        "                    self.target_companies.add(line.strip())\n",
        "        else:\n",
        "            self.logger.warning(\"VIP company file missing. All records will be evaluated.\")\n",
        "\n",
        "    def steps(self):\n",
        "        return [\n",
        "            MRStep(mapper_init=self.load_target_companies,\n",
        "                   mapper=self.map_customer_records),\n",
        "            MRStep(reducer=self.compute_churn_percentage)\n",
        "        ]\n",
        "\n",
        "    def map_customer_records(self, _, record):\n",
        "        \"\"\"\n",
        "        Mapper logic:\n",
        "        Emits (Company, ('total', 1)) for every record.\n",
        "        Emits (Company, ('churned', 1)) when the customer has churned.\n",
        "        Filters input based on the provided VIP company list, if available.\n",
        "        \"\"\"\n",
        "        if record.startswith(\"Customer ID\"):\n",
        "            return  # Skip header\n",
        "\n",
        "        try:\n",
        "            row = next(csv.reader([record]))\n",
        "            company_name = row[1]\n",
        "            churn_status = int(row[3])\n",
        "\n",
        "            if not self.target_companies or company_name in self.target_companies:\n",
        "                yield company_name, ('total', 1)\n",
        "                if churn_status == 1:\n",
        "                    yield company_name, ('churned', 1)\n",
        "        except (ValueError, IndexError):\n",
        "            self.increment_counter('CompanyChurnRateJob', 'CorruptedInputLines', 1)\n",
        "\n",
        "    def compute_churn_percentage(self, company, stats):\n",
        "        \"\"\"\n",
        "        Reducer logic:\n",
        "        Aggregates 'total' and 'churned' counts and computes churn ratio.\n",
        "        \"\"\"\n",
        "        total_users = 0\n",
        "        churned_users = 0\n",
        "\n",
        "        for entry_type, count in stats:\n",
        "            if entry_type == 'total':\n",
        "                total_users += count\n",
        "            elif entry_type == 'churned':\n",
        "                churned_users += count\n",
        "\n",
        "        churn_ratio = float(churned_users) / total_users if total_users > 0 else 0.0\n",
        "        yield company, f\"{churn_ratio:.4f}\"\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    CompanyChurnRateJob.run()\n"
      ],
      "metadata": {
        "id": "qSTFzX4_6lpX",
        "outputId": "684793fa-5e8f-4917-f2e3-68cd5127991c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing churn_rate_analysis.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Inline Test Output for Cruise Aggregation\n",
        "\n",
        "Below is the result of running the cruise aggregation job on sample input data."
      ],
      "metadata": {
        "id": "avJRgnSH68_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vip_companies.txt\n",
        "AlphaCorp\n",
        "BetaCorp\n",
        "DeltaCorp"
      ],
      "metadata": {
        "id": "ge4apU_k6-Te",
        "outputId": "595f4c9a-97a7-455c-d93e-1fb6a989313d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing vip_companies.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: Customer Churn by Company\n",
        "\n",
        "This task analyzes churned customers grouped by their associated companies using MapReduce."
      ],
      "metadata": {
        "id": "FhZtqiAQ7ErA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a compact sample churn dataset for testing\n",
        "demo_churn_content = \"\"\"Customer ID,Company,Region,Churn\n",
        "C001,AlphaCorp,North,0\n",
        "C002,BetaCorp,South,1\n",
        "C003,AlphaCorp,East,1\n",
        "C004,GammaCorp,West,0\n",
        "C005,BetaCorp,North,0\n",
        "C006,AlphaCorp,Central,0\n",
        "C007,BetaCorp,South,1\n",
        "C008,AlphaCorp,South,1\n",
        "C009,DeltaCorp,East,0\n",
        "C010,BetaCorp,West,0\n",
        "\"\"\"\n",
        "\n",
        "# Write the test data to a file\n",
        "with open(\"demo_churn_data.csv\", \"w\") as file:\n",
        "    file.write(demo_churn_content)\n",
        "\n",
        "print(\"Executing churn analysis on demo_churn_data.csv with a list of selected companies (VIP):\")\n",
        "\n",
        "# Use --files to include vip_companies.txt in the distributed cache\n",
        "# Pass the file using --vip-companies so the MRJob script can access it\n",
        "!python churn_rate_analysis.py demo_churn_data.csv --files vip_companies.txt --vip-companies vip_companies.txt\n"
      ],
      "metadata": {
        "id": "xDR3RDfA7FhO",
        "outputId": "f77aa3ed-ee90-4b19-8f58-965c5bc83409",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing churn analysis on demo_churn_data.csv with a list of selected companies (VIP):\n",
            "No configs found; falling back on auto-configuration\n",
            "No configs specified for inline runner\n",
            "Creating temp directory /tmp/churn_rate_analysis.root.20250729.160510.273787\n",
            "Running step 1 of 2...\n",
            "Running step 2 of 2...\n",
            "job output is in /tmp/churn_rate_analysis.root.20250729.160510.273787/output\n",
            "Streaming final output from /tmp/churn_rate_analysis.root.20250729.160510.273787/output...\n",
            "\"DeltaCorp\"\t\"0.0000\"\n",
            "\"AlphaCorp\"\t\"0.5000\"\n",
            "\"BetaCorp\"\t\"0.5000\"\n",
            "Removing temp directory /tmp/churn_rate_analysis.root.20250729.160510.273787...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile top_spending_states_job.py\n",
        "from mrjob.job import MRJob\n",
        "from mrjob.step import MRStep\n",
        "import csv\n",
        "import re\n",
        "\n",
        "class TopStateSpenders(MRJob):\n",
        "    \"\"\"\n",
        "    This MapReduce job calculates the total yearly spending per state\n",
        "    based on customer address data, and then outputs the top 5 highest spending states.\n",
        "\n",
        "\n",
        "    This MapReduce job calculates the total yearly spending per state\n",
        "    based on customer address data, and then outputs the top 5 highest spending states.\n",
        "\n",
        "\n",
        "\n",
        "    This MapReduce job calculates the total yearly spending per state\n",
        "    based on customer address data, and then outputs the top 5 highest spending states.\n",
        "    \"\"\"\n",
        "\n",
        "    def steps(self):\n",
        "        return [\n",
        "            MRStep(mapper=self.extract_state_and_spending,\n",
        "                   reducer=self.aggregate_spending_by_state),\n",
        "            MRStep(reducer=self.select_top_spending_states)\n",
        "        ]\n",
        "\n",
        "    def extract_state_and_spending(self, _, line):\n",
        "        \"\"\"\n",
        "        Mapper:\n",
        "        Extracts U.S. state codes and corresponding yearly spend from customer address records.\n",
        "        Emits: (State, YearlyAmountSpent)\n",
        "\n",
        "\n",
        "\n",
        "         Mapper:\n",
        "        Extracts U.S. state codes and corresponding yearly spend from customer address records.\n",
        "        Emits: (State, YearlyAmountSpent)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "         Mapper:\n",
        "        Extracts U.S. state codes and corresponding yearly spend from customer address records.\n",
        "        Emits: (State, YearlyAmountSpent)\n",
        "        \"\"\"\n",
        "        if line.startswith(\"Email\"):\n",
        "            return  # Skip header row\n",
        "\n",
        "        try:\n",
        "            fields = next(csv.reader([line]))\n",
        "            address_field = fields[1]  # Address column\n",
        "            spending = float(fields[7])  # Yearly Amount Spent column\n",
        "\n",
        "            # Match a 2-letter state abbreviation followed by a ZIP code\n",
        "            match = re.search(r',\\s*([A-Z]{2})\\s*\\d{5}', address_field)\n",
        "            if match:\n",
        "                state = match.group(1)\n",
        "                yield state, spending\n",
        "            else:\n",
        "                self.increment_counter('TopStateSpenders', 'MissingStateCode', 1)\n",
        "        except (ValueError, IndexError, TypeError):\n",
        "            self.increment_counter('TopStateSpenders', 'InvalidLines', 1)\n",
        "\n",
        "    def aggregate_spending_by_state(self, state, values):\n",
        "        \"\"\"\n",
        "        Reducer (Step 1):\n",
        "\n",
        "        \"\"\"\n",
        "        total = sum(values)\n",
        "        yield None, (total, state)\n",
        "\n",
        "    def select_top_spending_states(self, _, state_spend_list):\n",
        "        \"\"\"\n",
        "        Reducer (Step 2):\n",
        "\n",
        "        \"\"\"\n",
        "        sorted_results = sorted(state_spend_list, key=lambda x: x[0], reverse=True)\n",
        "\n",
        "        for index, (total, state) in enumerate(sorted_results):\n",
        "            if index < 5:\n",
        "                yield state, round(total, 2)\n",
        "            else:\n",
        "                break\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    TopStateSpenders.run()\n"
      ],
      "metadata": {
        "id": "XOkR7PTe7rfa",
        "outputId": "4ea8fff8-0a24-44a6-8354-860982de357e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing top_spending_states_job.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inline Test Output for Customer Churn Analysis"
      ],
      "metadata": {
        "id": "uDFej7QY8uHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare mock E-commerce dataset with quoted address fields for testing\n",
        "test_ecom_data = \"\"\"Email,Address,Avatar,Avg. Session Length,Time on App,Time on Website,Length of Membership,Yearly Amount Spent\n",
        "cust1@example.com,\"123 Main St, Springfield, IL 62701\",20.0,3.5,0.0,12.0,500.25\n",
        "cust2@example.com,\"456 Oak Ave, Pleasantville, CA 90210\",30.5,4.0,2.1,24.5,1200.50\n",
        "cust3@example.com,\"789 Pine Ln, Metropolis, NY 10001\",25.1,2.8,1.5,18.0,800.75\n",
        "cust4@example.com,\"101 Elm Blvd, Springfield, IL 62701\",15.0,2.0,0.5,8.0,300.00\n",
        "cust5@example.com,\"202 Maple Dr, Sunnydale, CA 90210\",40.0,5.0,3.0,36.0,1500.00\n",
        "cust6@example.com,\"303 River Rd, Gotham, NY 10001\",22.5,3.1,1.0,15.0,950.00\n",
        "cust7@example.com,\"404 Hilltop, Smallville, KS 66044\",18.0,2.5,0.2,10.0,400.00\n",
        "cust8@example.com,\"505 Valley Dr, Central City, CA 90210\",35.0,4.5,2.5,30.0,1100.00\n",
        "cust9@example.com,\"606 Ocean Ave, Star City, WA 98001\",28.0,3.8,1.8,20.0,1300.00\n",
        "cust10@example.com,\"707 Mountain Rd, Riverdale, NY 10001\",10.0,1.5,0.0,5.0,200.00\n",
        "\"\"\"\n",
        "\n",
        "# Save data to file\n",
        "with open(\"test_ecommerce_data.csv\", \"w\") as file:\n",
        "    file.write(test_ecom_data)\n",
        "\n",
        "print(\"Test file 'test_ecommerce_data.csv' has been successfully created with properly quoted address fields.\")\n"
      ],
      "metadata": {
        "id": "bRB7QtBY8u1V",
        "outputId": "2e7748ce-9de0-4b7a-ecad-1bf1e77f0951",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test file 'test_ecommerce_data.csv' has been successfully created with properly quoted address fields.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: E-Commerce Customer Spending by Country\n",
        "\n",
        "This job computes spending insights from an e-commerce dataset using MapReduce."
      ],
      "metadata": {
        "id": "x4r8za6W9QKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Launching Top Spending States analysis using test_ecommerce_data.csv (sample run):\")\n",
        "!python top_spending_states_job.py test_ecommerce_data.csv\n"
      ],
      "metadata": {
        "id": "GV0EM1gN9VLg",
        "outputId": "42d7f4e1-16cc-4be8-f78c-8d967241a2d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Launching Top Spending States analysis using test_ecommerce_data.csv (sample run):\n",
            "No configs found; falling back on auto-configuration\n",
            "No configs specified for inline runner\n",
            "Creating temp directory /tmp/top_spending_states_job.root.20250729.161312.852051\n",
            "Running step 1 of 2...\n",
            "\n",
            "Counters: 1\n",
            "\tTopStateSpenders\n",
            "\t\tInvalidLines=10\n",
            "\n",
            "Running step 2 of 2...\n",
            "job output is in /tmp/top_spending_states_job.root.20250729.161312.852051/output\n",
            "Streaming final output from /tmp/top_spending_states_job.root.20250729.161312.852051/output...\n",
            "Removing temp directory /tmp/top_spending_states_job.root.20250729.161312.852051...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a small dummy input file for testing (WITH QUOTED ADDRESSES AND COMPLETE COLUMNS)\n",
        "test_ecommerce_data = \"\"\"Email,Address,Avatar,Avg. Session Length,Time on App,Time on Website,Length of Membership,Yearly Amount Spent\n",
        "cust1@example.com,\"123 Main St, Springfield, IL 62701\",Lavender,20.0,3.5,0.0,12.0,500.25\n",
        "cust2@example.com,\"456 Oak Ave, Pleasantville, CA 90210\",Teal,30.5,4.0,2.1,24.5,1200.50\n",
        "cust3@example.com,\"789 Pine Ln, Metropolis, NY 10001\",Blue,25.1,2.8,1.5,18.0,800.75\n",
        "cust4@example.com,\"101 Elm Blvd, Springfield, IL 62701\",Green,15.0,2.0,0.5,8.0,300.00\n",
        "cust5@example.com,\"202 Maple Dr, Sunnydale, CA 90210\",Red,40.0,5.0,3.0,36.0,1500.00\n",
        "cust6@example.com,\"303 River Rd, Gotham, NY 10001\",Orange,22.5,3.1,1.0,15.0,950.00\n",
        "cust7@example.com,\"404 Hilltop, Smallville, KS 66044\",Yellow,18.0,2.5,0.2,10.0,400.00\n",
        "cust8@example.com,\"505 Valley Dr, Central City, CA 90210\",Purple,35.0,4.5,2.5,30.0,1100.00\n",
        "cust9@example.com,\"606 Ocean Ave, Star City, WA 98001\",Gray,28.0,3.8,1.8,20.0,1300.00\n",
        "cust10@example.com,\"707 Mountain Rd, Riverdale, NY 10001\",Black,10.0,1.5,0.0,5.0,200.00\n",
        "\"\"\"\n",
        "\n",
        "with open(\"test_ecommerce_data.csv\", \"w\") as f:\n",
        "    f.write(test_ecommerce_data)\n",
        "\n",
        "print(\"✅ test_ecommerce_data.csv has been created with quoted addresses and full column data.\")\n"
      ],
      "metadata": {
        "id": "JIDYWEFg9f1j",
        "outputId": "cad0d74b-9553-4100-c34f-5e6df415ba30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ test_ecommerce_data.csv has been created with quoted addresses and full column data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inline Test Output for E-Commerce Spending Analysis"
      ],
      "metadata": {
        "id": "cvyFcvJK-QjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ship_filter_median_length_job.py\n",
        "from mrjob.job import MRJob\n",
        "from mrjob.step import MRStep\n",
        "import csv\n",
        "\n",
        "class MRShipFilterMedianLength(MRJob):\n",
        "    \"\"\"\n",
        "  n.\n",
        "    \"\"\"\n",
        "\n",
        "    def steps(self):\n",
        "        return [\n",
        "            MRStep(mapper=self.mapper_filter_ships,\n",
        "                   reducer=self.reducer_compute_median_length)\n",
        "        ]\n",
        "\n",
        "    def mapper_filter_ships(self, _, line):\n",
        "        \"\"\"\n",
        "       .\n",
        "        \"\"\"\n",
        "        if line.startswith(\"Cruise_line\"):  # Skip header\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            # Using csv.reader for robust parsing\n",
        "            row = next(csv.reader([line]))\n",
        "            cruise_line = row[0]\n",
        "            passenger_density = float(row[8])  # Passenger_density is 9th column (index 8)\n",
        "            length = float(row[9])  # Length is 10th column (index 9)\n",
        "\n",
        "            if passenger_density > 35.0:\n",
        "                yield cruise_line, length\n",
        "        except (ValueError, IndexError):\n",
        "            self.increment_counter('MRShipFilterMedianLength', 'Bad CSV lines', 1)\n",
        "            pass  # Skip malformed lines\n",
        "\n",
        "    def reducer_compute_median_length(self, cruise_line, lengths):\n",
        "        \"\"\"\n",
        "        Reducer: Computes the median length for each cruise line.\n",
        "        \"\"\"\n",
        "        all_lengths = sorted(lengths)\n",
        "        n = len(all_lengths)\n",
        "\n",
        "        if n == 0:\n",
        "            median = 0.0\n",
        "        elif n % 2 == 1:\n",
        "            median = all_lengths[n // 2]\n",
        "        else:\n",
        "            mid1 = all_lengths[n // 2 - 1]\n",
        "            mid2 = all_lengths[n // 2]\n",
        "            median = (mid1 + mid2) / 2.0\n",
        "\n",
        "        yield cruise_line, round(median, 2)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    MRShipFilterMedianLength.run()\n"
      ],
      "metadata": {
        "id": "SMmiow7y-ReH",
        "outputId": "48c48215-ca25-4662-ec31-a27eb5425831",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ship_filter_median_length_job.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a small dummy cruise.csv file for testing\n",
        "small_cruise_data = \"\"\"Cruise_line,Cruise_ship_name,Tonnage,passengers,crew,built,Inaugural_Date,Years_in_service,Passenger_density,length,cabins\n",
        "AIDA Cruises,AIDAbella,69203,2050,600,2008,2008,12,33.75,252.0,1025\n",
        "AIDA Cruises,AIDAluna,69203,2050,600,2009,2009,11,33.75,252.0,1025\n",
        "Carnival Cruise Line,Carnival Freedom,110000,2974,1150,2007,2007,13,37.00,290.0,1487\n",
        "Carnival Cruise Line,Carnival Horizon,133500,3960,1450,2018,2018,2,33.71,323.0,1980\n",
        "Royal Caribbean,Allure of the Seas,225282,5400,2200,2010,2010,10,41.67,362.0,2700\n",
        "\"\"\"\n",
        "\n",
        "with open(\"small_cruise.csv\", \"w\") as f:\n",
        "    f.write(small_cruise_data)\n",
        "\n",
        "print(\"Running Ship Filter & Median Length Job on small_cruise.csv (inline test output):\")\n",
        "!python ship_filter_median_length_job.py small_cruise.csv\n"
      ],
      "metadata": {
        "id": "Ig2QLeD0-ihy",
        "outputId": "f0840491-d58c-4a6a-acb4-aa4043abca97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Ship Filter & Median Length Job on small_cruise.csv (inline test output):\n",
            "No configs found; falling back on auto-configuration\n",
            "No configs specified for inline runner\n",
            "Creating temp directory /tmp/ship_filter_median_length_job.root.20250729.161840.608725\n",
            "Running step 1 of 1...\n",
            "job output is in /tmp/ship_filter_median_length_job.root.20250729.161840.608725/output\n",
            "Streaming final output from /tmp/ship_filter_median_length_job.root.20250729.161840.608725/output...\n",
            "\"Carnival Cruise Line\"\t290.0\n",
            "\"Royal Caribbean\"\t362.0\n",
            "Removing temp directory /tmp/ship_filter_median_length_job.root.20250729.161840.608725...\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}