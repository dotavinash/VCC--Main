{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dotavinash/VCC--Main/blob/main/Assignment2_G24AI2025_AvinashKumar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New section"
      ],
      "metadata": {
        "id": "UT4QpvgaJwqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install mrjob\n",
        "!pip install mrjob\n",
        "\n",
        "# Install Java (required for Hadoop)\n",
        "!apt-get install openjdk-8-jdk -qq > /dev/null\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "\n",
        "# Download and setup Hadoop (using a recent stable version, e.g., 3.3.6)\n",
        "HADOOP_VERSION = \"3.3.6\"\n",
        "!wget -q https://archive.apache.org/dist/hadoop/common/hadoop-{HADOOP_VERSION}/hadoop-{HADOOP_VERSION}.tar.gz\n",
        "!tar -xzf hadoop-{HADOOP_VERSION}.tar.gz\n",
        "!mv hadoop-{HADOOP_VERSION} /usr/local/hadoop\n",
        "\n",
        "# Set Hadoop environment variables\n",
        "os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop\"\n",
        "os.environ[\"PATH\"] = os.environ[\"PATH\"] + \":\" + os.environ[\"HADOOP_HOME\"] + \"/bin\"\n",
        "\n",
        "# Handle CLASSPATH: Check if it exists before appending, otherwise initialize it.\n",
        "if \"CLASSPATH\" in os.environ:\n",
        "    os.environ[\"CLASSPATH\"] = os.environ[\"CLASSPATH\"] + \":\" + os.environ[\"HADOOP_HOME\"] + \"/lib/*\"\n",
        "else:\n",
        "    os.environ[\"CLASSPATH\"] = os.environ[\"HADOOP_HOME\"] + \"/lib/*\"\n",
        "\n",
        "print(\"Dependencies and Hadoop setup complete!\")"
      ],
      "metadata": {
        "id": "NGT1tLa9JxnH",
        "outputId": "4109e22b-4fbd-4277-fe8c-81d3a82bf3b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mrjob\n",
            "  Downloading mrjob-0.7.4-py2.py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.11/dist-packages (from mrjob) (6.0.2)\n",
            "Downloading mrjob-0.7.4-py2.py3-none-any.whl (439 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/439.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.7/439.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m337.9/439.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.6/439.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mrjob\n",
            "Successfully installed mrjob-0.7.4\n",
            "Dependencies and Hadoop setup complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CSV URLs ---\n",
        "CRUISE_URL = \"https://raw.githubusercontent.com/TakMashhido/PGD-BigData-Tutorial/refs/heads/main/Dataset/cruise.csv\"\n",
        "CHURN_URL = \"https://raw.githubusercontent.com/TakMashhido/PGD-BigData-Tutorial/refs/heads/main/Dataset/customer_churn.csv\"\n",
        "ECOMMERCE_URL = \"https://raw.githubusercontent.com/TakMashhido/PGD-BigData-Tutorial/refs/heads/main/Dataset/e-com_customer.csv\"\n",
        "\n",
        "# Define local filenames for consistency\n",
        "CRUISE_FILE = \"cruise.csv\"\n",
        "CHURN_FILE = \"customer_churn.csv\"\n",
        "ECOMMERCE_FILE = \"e_commerce_customer.csv\"\n",
        "\n",
        "print(\"Downloading files from provided URLs...\")\n",
        "\n",
        "# Download Cruise data\n",
        "!wget -O {CRUISE_FILE} {CRUISE_URL}\n",
        "\n",
        "# Download Customer Churn data\n",
        "!wget -O {CHURN_FILE} {CHURN_URL}\n",
        "\n",
        "# Download E-commerce Customer data\n",
        "!wget -O {ECOMMERCE_FILE} {ECOMMERCE_URL}\n",
        "\n",
        "print(\"\\nCSV files loaded/prepared!\")\n",
        "!ls -lh *.csv"
      ],
      "metadata": {
        "id": "eBbPvo1_J7rY",
        "outputId": "bf0c21eb-4582-4785-c66b-1cd15e62e198",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading files from provided URLs...\n",
            "--2025-07-28 17:52:50--  https://raw.githubusercontent.com/TakMashhido/PGD-BigData-Tutorial/refs/heads/main/Dataset/cruise.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8734 (8.5K) [text/plain]\n",
            "Saving to: ‘cruise.csv’\n",
            "\n",
            "cruise.csv          100%[===================>]   8.53K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2025-07-28 17:52:50 (16.2 MB/s) - ‘cruise.csv’ saved [8734/8734]\n",
            "\n",
            "--2025-07-28 17:52:50--  https://raw.githubusercontent.com/TakMashhido/PGD-BigData-Tutorial/refs/heads/main/Dataset/customer_churn.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 115479 (113K) [text/plain]\n",
            "Saving to: ‘customer_churn.csv’\n",
            "\n",
            "customer_churn.csv  100%[===================>] 112.77K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-07-28 17:52:50 (3.33 MB/s) - ‘customer_churn.csv’ saved [115479/115479]\n",
            "\n",
            "--2025-07-28 17:52:50--  https://raw.githubusercontent.com/TakMashhido/PGD-BigData-Tutorial/refs/heads/main/Dataset/e-com_customer.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 86871 (85K) [text/plain]\n",
            "Saving to: ‘e_commerce_customer.csv’\n",
            "\n",
            "e_commerce_customer 100%[===================>]  84.83K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-07-28 17:52:51 (2.53 MB/s) - ‘e_commerce_customer.csv’ saved [86871/86871]\n",
            "\n",
            "\n",
            "CSV files loaded/prepared!\n",
            "-rw-r--r-- 1 root root 8.6K Jul 28 17:52 cruise.csv\n",
            "-rw-r--r-- 1 root root 113K Jul 28 17:52 customer_churn.csv\n",
            "-rw-r--r-- 1 root root  85K Jul 28 17:52 e_commerce_customer.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: Cruise Line Aggregations\n",
        "\n",
        "This task involves performing aggregations (e.g., total passengers and tonnage) by cruise line using MapReduce."
      ],
      "metadata": {
        "id": "L9pGD6Q5Li9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile cruise_aggregations_job.py\n",
        "from mrjob.job import MRJob\n",
        "from mrjob.step import MRStep\n",
        "import csv\n",
        "\n",
        "class MRCruiseAggregations(MRJob):\n",
        "    \"\"\"\n",
        "    Computes total ships, average tonnage, and maximum crew size for each cruise line.\n",
        "    Uses a combiner for partial aggregation to improve efficiency.\n",
        "    \"\"\"\n",
        "\n",
        "    def steps(self):\n",
        "        return [\n",
        "            MRStep(mapper=self.mapper_get_cruise_data,\n",
        "                   combiner=self.combiner_partial_aggregations,\n",
        "                   reducer=self.reducer_final_aggregations)\n",
        "        ]\n",
        "\n",
        "    def mapper_get_cruise_data(self, _, line):\n",
        "        \"\"\"\n",
        "        Mapper: Parses each line of the cruise CSV.\n",
        "        Emits (Cruise_line, (1, Tonnage, Crew)).\n",
        "        '1' for counting ships, Tonnage for sum, Crew for max.\n",
        "        \"\"\"\n",
        "        # Skip header\n",
        "        if line.startswith(\"Cruise_line\"):\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            # Using csv.reader for robust parsing, especially with commas in data\n",
        "            # csv.reader expects an iterable of lines, so [line] wraps the current line.\n",
        "            row = next(csv.reader([line]))\n",
        "            cruise_line = row[0]\n",
        "            # Tonnage is at index 2, crew at index 4 (0-indexed)\n",
        "            tonnage = float(row[2])\n",
        "            crew = int(row[4])\n",
        "            # Emit: (count of ships, sum of tonnage, max crew size)\n",
        "            yield cruise_line, (1, tonnage, crew)\n",
        "        except (ValueError, IndexError) as e:\n",
        "            # Increment a counter for bad lines to monitor data quality\n",
        "            self.increment_counter('MRCruiseAggregations', 'Bad CSV lines', 1)\n",
        "            # print(f\"Skipping malformed line: {line} - Error: {e}\") # Uncomment for debugging\n",
        "            pass\n",
        "\n",
        "    def combiner_partial_aggregations(self, cruise_line, values):\n",
        "        \"\"\"\n",
        "        Combiner: Performs partial aggregation (sum for count/tonnage, max for crew)\n",
        "        before sending to the reducer. This reduces data shuffled across the network.\n",
        "        \"\"\"\n",
        "        total_ships = 0\n",
        "        total_tonnage = 0.0\n",
        "        max_crew = 0\n",
        "\n",
        "        for count, tonnage, crew in values:\n",
        "            total_ships += count\n",
        "            total_tonnage += tonnage\n",
        "            max_crew = max(max_crew, crew)\n",
        "        yield cruise_line, (total_ships, total_tonnage, max_crew)\n",
        "\n",
        "    def reducer_final_aggregations(self, cruise_line, values):\n",
        "        \"\"\"\n",
        "        Reducer: Aggregates the partial results from combiners/mappers.\n",
        "        Computes final total ships, average tonnage, and maximum crew size.\n",
        "        \"\"\"\n",
        "        total_ships = 0\n",
        "        total_tonnage = 0.0\n",
        "        max_crew = 0\n",
        "\n",
        "        for count, tonnage, crew in values:\n",
        "            total_ships += count\n",
        "            total_tonnage += tonnage\n",
        "            max_crew = max(max_crew, crew)\n",
        "\n",
        "        avg_tonnage = total_tonnage / total_ships if total_ships > 0 else 0.0\n",
        "        yield cruise_line, (total_ships, round(avg_tonnage, 2), max_crew)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    MRCruiseAggregations.run()"
      ],
      "metadata": {
        "id": "D6ESBA_6Kqld",
        "outputId": "9ca7fcb9-a377-4346-fc21-559ea570a01c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cruise_aggregations_job.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a small dummy input file for demonstration\n",
        "small_cruise_data = \"\"\"Cruise_line,Cruise_ship_name,Tonnage,passengers,crew,built,Inaugural_Date,Years_in_service,Passenger_density,length,cabins\n",
        "AIDA Cruises,AIDAbella,69203,2050,600,2008,2008,12,33.75,252.0,1025\n",
        "AIDA Cruises,AIDAluna,69203,2050,600,2009,2009,11,33.75,252.0,1025\n",
        "Carnival Cruise Line,Carnival Freedom,110000,2974,1150,2007,2007,13,37.00,290.0,1487\n",
        "Carnival Cruise Line,Carnival Horizon,133500,3960,1450,2018,2018,2,33.71,323.0,1980\n",
        "Royal Caribbean,Allure of the Seas,225282,5400,2200,2010,2010,10,41.67,362.0,2700\n",
        "\"\"\"\n",
        "with open(\"small_cruise.csv\", \"w\") as f:\n",
        "    f.write(small_cruise_data)\n",
        "\n",
        "print(\"Running Cruise Aggregations Job on small_cruise.csv (inline test output):\")\n",
        "!python cruise_aggregations_job.py small_cruise.csv"
      ],
      "metadata": {
        "id": "n_fjoEkKKyWT",
        "outputId": "0e09c9b2-85f5-4822-9a8f-b493bb914e31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Cruise Aggregations Job on small_cruise.csv (inline test output):\n",
            "No configs found; falling back on auto-configuration\n",
            "No configs specified for inline runner\n",
            "Creating temp directory /tmp/cruise_aggregations_job.root.20250728.175349.326046\n",
            "Running step 1 of 1...\n",
            "job output is in /tmp/cruise_aggregations_job.root.20250728.175349.326046/output\n",
            "Streaming final output from /tmp/cruise_aggregations_job.root.20250728.175349.326046/output...\n",
            "\"Royal Caribbean\"\t[1, 225282.0, 2200]\n",
            "\"AIDA Cruises\"\t[2, 69203.0, 600]\n",
            "\"Carnival Cruise Line\"\t[2, 121750.0, 1450]\n",
            "Removing temp directory /tmp/cruise_aggregations_job.root.20250728.175349.326046...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile company_churn_job.py\n",
        "from mrjob.job import MRJob\n",
        "from mrjob.step import MRStep\n",
        "import csv\n",
        "\n",
        "class MRCompanyChurn(MRJob):\n",
        "    \"\"\"\n",
        "    Computes churn rate for specified companies from customer_churn.csv.\n",
        "    Uses a two-step pipeline:\n",
        "    Step 1: Mapper emits (Company, 'total') and (Company, 'churned')\n",
        "            if the company is in the VIP list from distributed cache.\n",
        "    Step 2: Reducer calculates churn rate (CHURNED / TOTAL) for each company.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define files to be placed in the distributed cache\n",
        "    def configure_args(self):\n",
        "        super(MRCompanyChurn, self).configure_args()\n",
        "        self.add_file_arg('--vip-companies', help='Path to VIP companies list')\n",
        "\n",
        "    def load_vip_companies(self):\n",
        "        \"\"\"\n",
        "        Mapper initialization: Loads VIP company names from the distributed cache.\n",
        "        This runs once per mapper process.\n",
        "        \"\"\"\n",
        "        self.vip_companies = set()\n",
        "        # self.options.vip_companies will contain the path to the file in the distributed cache\n",
        "        if self.options.vip_companies:\n",
        "            with open(self.options.vip_companies, 'r') as f:\n",
        "                for line in f:\n",
        "                    self.vip_companies.add(line.strip())\n",
        "        else:\n",
        "            # This warning appears if --vip-companies is not provided\n",
        "            self.logger.warning(\"No VIP companies file provided. Processing all companies.\")\n",
        "\n",
        "\n",
        "    def steps(self):\n",
        "        return [\n",
        "            MRStep(mapper_init=self.load_vip_companies, # This runs once before the mapper starts processing data\n",
        "                   mapper=self.mapper_churn_counts),\n",
        "            MRStep(reducer=self.reducer_churn_rate)\n",
        "        ]\n",
        "\n",
        "    def mapper_churn_counts(self, _, line):\n",
        "        \"\"\"\n",
        "        Mapper: Parses customer churn data.\n",
        "        Emits (Company, 'total_count') for every record.\n",
        "        Emits (Company, 'churned_count') if Churn == 1.\n",
        "        Only processes companies present in the VIP list (if provided via distributed cache).\n",
        "        \"\"\"\n",
        "        if line.startswith(\"Customer ID\"): # Skip header\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            row = next(csv.reader([line]))\n",
        "            # Assuming format: Customer ID,Company,Region,Churn\n",
        "            company = row[1]\n",
        "            churn = int(row[3])\n",
        "\n",
        "            # Filter by VIP companies if the list is loaded, otherwise process all (if vip_companies is empty)\n",
        "            if not self.vip_companies or company in self.vip_companies:\n",
        "                yield company, ('total', 1)\n",
        "                if churn == 1:\n",
        "                    yield company, ('churned', 1)\n",
        "        except (ValueError, IndexError) as e:\n",
        "            self.increment_counter('MRCompanyChurn', 'Bad CSV lines', 1)\n",
        "            # print(f\"Skipping malformed line: {line} - Error: {e}\") # Uncomment for debugging\n",
        "            pass\n",
        "\n",
        "    def reducer_churn_rate(self, company, counts):\n",
        "        \"\"\"\n",
        "        Reducer: Aggregates counts and calculates the churn rate for each company.\n",
        "        \"\"\"\n",
        "        total_customers = 0\n",
        "        churned_customers = 0\n",
        "\n",
        "        for count_type, value in counts:\n",
        "            if count_type == 'total':\n",
        "                total_customers += value\n",
        "            elif count_type == 'churned':\n",
        "                churned_customers += value\n",
        "\n",
        "        churn_rate = 0.0\n",
        "        if total_customers > 0:\n",
        "            churn_rate = float(churned_customers) / total_customers\n",
        "\n",
        "        yield company, f\"{churn_rate:.4f}\" # Output as four-decimal float\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    MRCompanyChurn.run()"
      ],
      "metadata": {
        "id": "D8xzgo5nK47Z",
        "outputId": "84beb3d0-a6bd-4359-9ed5-2c0fdd4be8cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing company_churn_job.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "Inline Test Output for Cruise Aggregation\n",
        "\n",
        "Below is the result of running the cruise aggregation job on sample input data."
      ],
      "metadata": {
        "id": "M7E_habpLtFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vip_companies.txt\n",
        "AlphaCorp\n",
        "BetaCorp\n",
        "DeltaCorp"
      ],
      "metadata": {
        "id": "Ruj3dLlLK8oX",
        "outputId": "a0769722-796c-4277-a8cc-62b68edb239f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing vip_companies.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: Customer Churn by Company\n",
        "\n",
        "This task analyzes churned customers grouped by their associated companies using MapReduce."
      ],
      "metadata": {
        "id": "qzerKDwuLzUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a small dummy input file for demonstration\n",
        "small_churn_data = \"\"\"Customer ID,Company,Region,Churn\n",
        "C001,AlphaCorp,North,0\n",
        "C002,BetaCorp,South,1\n",
        "C003,AlphaCorp,East,1\n",
        "C004,GammaCorp,West,0\n",
        "C005,BetaCorp,North,0\n",
        "C006,AlphaCorp,Central,0\n",
        "C007,BetaCorp,South,1\n",
        "C008,AlphaCorp,South,1\n",
        "C009,DeltaCorp,East,0\n",
        "C010,BetaCorp,West,0\n",
        "\"\"\"\n",
        "with open(\"small_customer_churn.csv\", \"w\") as f:\n",
        "    f.write(small_churn_data)\n",
        "\n",
        "print(\"Running Company Churn Rate Job on small_customer_churn.csv with VIP companies (inline test output):\")\n",
        "# Use --files to put vip_companies.txt into the distributed cache (required by Hadoop)\n",
        "# Use --vip-companies to tell your mrjob script the name of the file it needs to open\n",
        "!python company_churn_job.py small_customer_churn.csv --files vip_companies.txt --vip-companies vip_companies.txt"
      ],
      "metadata": {
        "id": "2K4qM1A4LAaT",
        "outputId": "ab94c0dd-037f-4393-ada3-5317548361a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Company Churn Rate Job on small_customer_churn.csv with VIP companies (inline test output):\n",
            "No configs found; falling back on auto-configuration\n",
            "No configs specified for inline runner\n",
            "Creating temp directory /tmp/company_churn_job.root.20250728.175436.624120\n",
            "Running step 1 of 2...\n",
            "Running step 2 of 2...\n",
            "job output is in /tmp/company_churn_job.root.20250728.175436.624120/output\n",
            "Streaming final output from /tmp/company_churn_job.root.20250728.175436.624120/output...\n",
            "\"DeltaCorp\"\t\"0.0000\"\n",
            "\"AlphaCorp\"\t\"0.5000\"\n",
            "\"BetaCorp\"\t\"0.5000\"\n",
            "Removing temp directory /tmp/company_churn_job.root.20250728.175436.624120...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile state_spending_job.py\n",
        "from mrjob.job import MRJob\n",
        "from mrjob.step import MRStep\n",
        "import csv\n",
        "import re\n",
        "\n",
        "class MRStateSpending(MRJob):\n",
        "    \"\"\"\n",
        "    Computes total yearly amount spent per state from e-commerce customer data\n",
        "    and then outputs the top 5 states by spending.\n",
        "    \"\"\"\n",
        "\n",
        "    def steps(self):\n",
        "        return [\n",
        "            # Step 1: Map (extract state & spending) -> Reduce (sum spending per state)\n",
        "            MRStep(mapper=self.mapper_get_state_spending,\n",
        "                   reducer=self.reducer_sum_spending_per_state),\n",
        "            # Step 2: Reduce (collect all sums, sort, and find top 5)\n",
        "            MRStep(reducer=self.reducer_find_top_states)\n",
        "        ]\n",
        "\n",
        "    def mapper_get_state_spending(self, _, line):\n",
        "        \"\"\"\n",
        "        Mapper: Parses the E-commerce Customer CSV, extracts the state code\n",
        "        from the Address, and emits (State, Yearly Amount Spent).\n",
        "        \"\"\"\n",
        "        if line.startswith(\"Email\"): # Skip header line\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            row = next(csv.reader([line]))\n",
        "            # Assuming columns are: Email,Address,Avatar,Avg. Session Length,Time on App,Time on Website,Length of Membership,Yearly Amount Spent\n",
        "            address = row[1] # Address is the second column (index 1)\n",
        "            yearly_amount_spent = float(row[7]) # Yearly Amount Spent is the eighth column (index 7)\n",
        "\n",
        "            # Regex to find a two-letter uppercase state code followed by a 5-digit zip code\n",
        "            # Example: \"123 Main St, Anytown, CA 90210\" -> extracts \"CA\"\n",
        "            match = re.search(r',\\s*([A-Z]{2})\\s*\\d{5}', address)\n",
        "            if match:\n",
        "                state_code = match.group(1) # Extract the two-letter state code\n",
        "                yield state_code, yearly_amount_spent\n",
        "            else:\n",
        "                self.increment_counter('MRStateSpending', 'No state code found', 1)\n",
        "        except (ValueError, IndexError, TypeError) as e:\n",
        "            # Catch errors for malformed lines or incorrect data types\n",
        "            self.increment_counter('MRStateSpending', 'Bad CSV lines', 1)\n",
        "            # print(f\"Skipping malformed line: {line} - Error: {e}\") # Uncomment for debugging\n",
        "            pass\n",
        "\n",
        "    def reducer_sum_spending_per_state(self, state, amounts):\n",
        "        \"\"\"\n",
        "        Reducer 1: Sums the yearly amount spent for each state.\n",
        "        Emits (None, (total_spending, state)) to prepare for a global sort\n",
        "        in the next reducer step.\n",
        "        \"\"\"\n",
        "        total_spending = sum(amounts)\n",
        "        # Emit with a None key so all pairs go to a single reducer in the next step\n",
        "        yield None, (total_spending, state)\n",
        "\n",
        "    def reducer_find_top_states(self, _, spending_state_pairs):\n",
        "        \"\"\"\n",
        "        Reducer 2: Collects all (total_spending, state) pairs, sorts them globally,\n",
        "        and yields only the top 5 states by spending.\n",
        "        \"\"\"\n",
        "        # Collect all (spending, state) tuples and sort in descending order of spending\n",
        "        sorted_states = sorted(spending_state_pairs, key=lambda x: x[0], reverse=True)\n",
        "\n",
        "        # Emit only the top 5 states\n",
        "        for i, (total_spending, state) in enumerate(sorted_states):\n",
        "            if i < 5:\n",
        "                yield state, round(total_spending, 2)\n",
        "            else:\n",
        "                break # Stop after emitting the top 5\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    MRStateSpending.run()"
      ],
      "metadata": {
        "id": "-s9Kn1OrLEcX",
        "outputId": "68614665-d88d-4664-ebcf-82b008a64309",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing state_spending_job.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inline Test Output for Customer Churn Analysis"
      ],
      "metadata": {
        "id": "ZtTIPp6KLNI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a small dummy input file for demonstration (WITH QUOTED ADDRESSES)\n",
        "small_ecommerce_data = \"\"\"Email,Address,Avatar,Avg. Session Length,Time on App,Time on Website,Length of Membership,Yearly Amount Spent\n",
        "cust1@example.com,\"123 Main St, Springfield, IL 62701\",20.0,3.5,0.0,12.0,500.25\n",
        "cust2@example.com,\"456 Oak Ave, Pleasantville, CA 90210\",30.5,4.0,2.1,24.5,1200.50\n",
        "cust3@example.com,\"789 Pine Ln, Metropolis, NY 10001\",25.1,2.8,1.5,18.0,800.75\n",
        "cust4@example.com,\"101 Elm Blvd, Springfield, IL 62701\",15.0,2.0,0.5,8.0,300.00\n",
        "cust5@example.com,\"202 Maple Dr, Sunnydale, CA 90210\",40.0,5.0,3.0,36.0,1500.00\n",
        "cust6@example.com,\"303 River Rd, Gotham, NY 10001\",22.5,3.1,1.0,15.0,950.00\n",
        "cust7@example.com,\"404 Hilltop, Smallville, KS 66044\",18.0,2.5,0.2,10.0,400.00\n",
        "cust8@example.com,\"505 Valley Dr, Central City, CA 90210\",35.0,4.5,2.5,30.0,1100.00\n",
        "cust9@example.com,\"606 Ocean Ave, Star City, WA 98001\",28.0,3.8,1.8,20.0,1300.00\n",
        "cust10@example.com,\"707 Mountain Rd, Riverdale, NY 10001\",10.0,1.5,0.0,5.0,200.00\n",
        "\"\"\"\n",
        "with open(\"small_e_commerce_customer.csv\", \"w\") as f:\n",
        "    f.write(small_ecommerce_data)\n",
        "\n",
        "print(\"small_e_commerce_customer.csv has been updated with quoted addresses.\")\n",
        "\n",
        "# Optional: Verify the file content by reading it back\n",
        "# with open(\"small_e_commerce_customer.csv\", \"r\") as f:\n",
        "#     print(\"\\nContent of updated small_e_commerce_customer.csv:\")\n",
        "#     print(f.read())"
      ],
      "metadata": {
        "id": "KrDM3QGwLJvb",
        "outputId": "bc802b3a-cf7b-4831-85c1-3df6ff696c3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "small_e_commerce_customer.csv has been updated with quoted addresses.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: E-Commerce Customer Spending by Country\n",
        "\n",
        "This job computes spending insights from an e-commerce dataset using MapReduce."
      ],
      "metadata": {
        "id": "QUTjKz6fNqbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Running State-wise Spending Job on small_e_commerce_customer.csv (inline test output):\")\n",
        "!python state_spending_job.py small_e_commerce_customer.csv"
      ],
      "metadata": {
        "id": "Eqhag0zmLO9A",
        "outputId": "82f1acac-dde3-4084-dcb0-4508b4f5b06f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running State-wise Spending Job on small_e_commerce_customer.csv (inline test output):\n",
            "No configs found; falling back on auto-configuration\n",
            "No configs specified for inline runner\n",
            "Creating temp directory /tmp/state_spending_job.root.20250728.175543.535204\n",
            "Running step 1 of 2...\n",
            "\n",
            "Counters: 1\n",
            "\tMRStateSpending\n",
            "\t\tBad CSV lines=10\n",
            "\n",
            "Running step 2 of 2...\n",
            "job output is in /tmp/state_spending_job.root.20250728.175543.535204/output\n",
            "Streaming final output from /tmp/state_spending_job.root.20250728.175543.535204/output...\n",
            "Removing temp directory /tmp/state_spending_job.root.20250728.175543.535204...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a small dummy input file for demonstration (WITH QUOTED ADDRESSES AND COMPLETE COLUMNS)\n",
        "small_ecommerce_data = \"\"\"Email,Address,Avatar,Avg. Session Length,Time on App,Time on Website,Length of Membership,Yearly Amount Spent\n",
        "cust1@example.com,\"123 Main St, Springfield, IL 62701\",Lavender,20.0,3.5,0.0,12.0,500.25\n",
        "cust2@example.com,\"456 Oak Ave, Pleasantville, CA 90210\",Teal,30.5,4.0,2.1,24.5,1200.50\n",
        "cust3@example.com,\"789 Pine Ln, Metropolis, NY 10001\",Blue,25.1,2.8,1.5,18.0,800.75\n",
        "cust4@example.com,\"101 Elm Blvd, Springfield, IL 62701\",Green,15.0,2.0,0.5,8.0,300.00\n",
        "cust5@example.com,\"202 Maple Dr, Sunnydale, CA 90210\",Red,40.0,5.0,3.0,36.0,1500.00\n",
        "cust6@example.com,\"303 River Rd, Gotham, NY 10001\",Orange,22.5,3.1,1.0,15.0,950.00\n",
        "cust7@example.com,\"404 Hilltop, Smallville, KS 66044\",Yellow,18.0,2.5,0.2,10.0,400.00\n",
        "cust8@example.com,\"505 Valley Dr, Central City, CA 90210\",Purple,35.0,4.5,2.5,30.0,1100.00\n",
        "cust9@example.com,\"606 Ocean Ave, Star City, WA 98001\",Gray,28.0,3.8,1.8,20.0,1300.00\n",
        "cust10@example.com,\"707 Mountain Rd, Riverdale, NY 10001\",Black,10.0,1.5,0.0,5.0,200.00\n",
        "\"\"\"\n",
        "with open(\"small_e_commerce_customer.csv\", \"w\") as f:\n",
        "    f.write(small_ecommerce_data)\n",
        "\n",
        "print(\"small_e_commerce_customer.csv has been updated with complete, quoted data.\")"
      ],
      "metadata": {
        "id": "yiijFl-0LTrW",
        "outputId": "b60884f2-e4ae-4fba-f177-a0bc42f80af2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "small_e_commerce_customer.csv has been updated with complete, quoted data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inline Test Output for E-Commerce Spending Analysis"
      ],
      "metadata": {
        "id": "MRGSNMiyNydg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ship_filter_median_length_job.py\n",
        "from mrjob.job import MRJob\n",
        "from mrjob.step import MRStep\n",
        "import csv\n",
        "import math\n",
        "\n",
        "class MRShipFilterMedianLength(MRJob):\n",
        "    \"\"\"\n",
        "    A two-step MapReduce pipeline on cruise.csv:\n",
        "    Step 1: Filters ships with passenger density > 35.0 and emits (Cruise line, length).\n",
        "    Step 2: Computes the median of the lengths for each Cruise line.\n",
        "    Handles both even and odd counts for median calculation.\n",
        "    \"\"\"\n",
        "\n",
        "    def steps(self):\n",
        "        return [\n",
        "            MRStep(mapper=self.mapper_filter_ships,\n",
        "                   # No combiner here as we need all lengths per key for median calculation\n",
        "                   reducer=self.reducer_compute_median_length)\n",
        "        ]\n",
        "\n",
        "    def mapper_filter_ships(self, _, line):\n",
        "        \"\"\"\n",
        "        Mapper: Filters ships based on passenger density.\n",
        "        Emits (Cruise_line, length) for ships with passenger density > 35.0.\n",
        "        \"\"\"\n",
        "        if line.startswith(\"Cruise_line\"): # Skip header\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            # Using csv.reader for robust parsing\n",
        "            row = next(csv.reader([line]))\n",
        "            cruise_line = row[0]\n",
        "            passenger_density = float(row[8]) # Passenger_density is 9th column (index 8)\n",
        "            length = float(row[9]) # length is 10th column (index 9)\n",
        "\n",
        "            if passenger_density > 35.0:\n",
        "                yield cruise_line, length\n",
        "        except (ValueError, IndexError) as e:\n",
        "            self.increment_counter('MRShipFilterMedianLength', 'Bad CSV lines', 1)\n",
        "            # print(f\"Skipping malformed line: {line} - Error: {e}\") # Uncomment for debugging\n",
        "            pass\n",
        "\n",
        "    def reducer_compute_median_length(self, cruise_line, lengths):\n",
        "        \"\"\"\n",
        "        Reducer: Computes the median length for each cruise line.\n",
        "        Handles even and odd counts for median calculation.\n",
        "        \"\"\"\n",
        "        # Collect all lengths and sort them to find the median\n",
        "        all_lengths = sorted(list(lengths))\n",
        "        n = len(all_lengths)\n",
        "\n",
        "        median = 0.0\n",
        "        if n == 0:\n",
        "            median = 0.0 # No lengths found for this cruise line after filtering\n",
        "        elif n % 2 == 1: # Odd number of elements\n",
        "            median = all_lengths[n // 2]\n",
        "        else: # Even number of elements\n",
        "            mid1 = all_lengths[n // 2 - 1]\n",
        "            mid2 = all_lengths[n // 2]\n",
        "            median = (mid1 + mid2) / 2.0\n",
        "\n",
        "        yield cruise_line, round(median, 2)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    MRShipFilterMedianLength.run()"
      ],
      "metadata": {
        "id": "l9xrMcxNLXKB",
        "outputId": "ce037fce-95d1-49bc-a640-76ccbc42625a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ship_filter_median_length_job.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "small_cruise_data = \"\"\"Cruise_line,Cruise_ship_name,Tonnage,passengers,crew,built,Inaugural_Date,Years_in_service,Passenger_density,length,cabins\n",
        "AIDA Cruises,AIDAbella,69203,2050,600,2008,2008,12,33.75,252.0,1025     # Density 33.75 <= 35.0 -> FILTERED OUT\n",
        "AIDA Cruises,AIDAluna,69203,2050,600,2009,2009,11,33.75,252.0,1025      # Density 33.75 <= 35.0 -> FILTERED OUT\n",
        "Carnival Cruise Line,Carnival Freedom,110000,2974,1150,2007,2007,13,37.00,290.0,1487 # Density 37.00 > 35.0 -> KEPT (length 290.0)\n",
        "Carnival Cruise Line,Carnival Horizon,133500,3960,1450,2018,2018,2,33.71,323.0,1980 # Density 33.71 <= 35.0 -> FILTERED OUT\n",
        "Royal Caribbean,Allure of the Seas,225282,5400,2200,2010,2010,10,41.67,362.0,2700 # Density 41.67 > 35.0 -> KEPT (length 362.0)\n",
        "\"\"\"\n",
        "# The small_cruise.csv file should already exist from previous questions.\n",
        "# If not, run the cell that defines and writes it in section 1.\n",
        "# Or explicitly create it again if you're only testing this part:\n",
        "with open(\"small_cruise.csv\", \"w\") as f:\n",
        "    f.write(small_cruise_data)\n",
        "\n",
        "print(\"Running Ship Filter & Median Length Job on small_cruise.csv (inline test output):\")\n",
        "!python ship_filter_median_length_job.py small_cruise.csv"
      ],
      "metadata": {
        "id": "VbwsjWjKLaMB",
        "outputId": "76dfd546-cb04-4f97-8d51-35c3fb2fecea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Ship Filter & Median Length Job on small_cruise.csv (inline test output):\n",
            "No configs found; falling back on auto-configuration\n",
            "No configs specified for inline runner\n",
            "Creating temp directory /tmp/ship_filter_median_length_job.root.20250728.175621.874089\n",
            "Running step 1 of 1...\n",
            "job output is in /tmp/ship_filter_median_length_job.root.20250728.175621.874089/output\n",
            "Streaming final output from /tmp/ship_filter_median_length_job.root.20250728.175621.874089/output...\n",
            "\"Carnival Cruise Line\"\t290.0\n",
            "\"Royal Caribbean\"\t362.0\n",
            "Removing temp directory /tmp/ship_filter_median_length_job.root.20250728.175621.874089...\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}